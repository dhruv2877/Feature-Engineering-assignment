{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Feature Engineering***\n",
        "\n",
        "* Theory questions\n"
      ],
      "metadata": {
        "id": "iAjBDjk4W4TO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Parameter?\n",
        "\n",
        " *   A parameter means a configuration variable that is internal to the model and whose value is estimated from the training data. Parameters are the elements the learning algorithm optimizes during the training process to improve model performance. These are not set manually, but are learned automatically.\n",
        "\n",
        "They directly affect the model’s predictions by shaping the learned patterns or relationships in the data. For instance, in a linear regression model, the slope and intercept are parameters that define the equation of the line fitted to the data.\n",
        "\n",
        "Parameters differ from hyperparameters, which are set before training and control the learning process itself (like learning rate, number of trees, etc.)."
      ],
      "metadata": {
        "id": "ydk2HGVFXZPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  What is correlation?\n",
        "   \n",
        "  What does negative correlation mean?\n",
        "\n",
        " *  Correlation is a statistical measure that expresses the extent to which two variables are linearly related. It indicates whether an increase or decrease in one variable would lead to an increase or decrease in another. The value of correlation lies between -1 and +1, where:\n",
        "\n",
        " *  +1 indicates a perfect positive correlation\n",
        "\n",
        " *  -1 indicates a perfect negative correlation\n",
        "\n",
        " *  0 indicates no linear correlation\n",
        "\n",
        "\n",
        "There are several types of correlation:\n",
        "\n",
        "1) Pearson correlation: Measures linear correlation.\n",
        "\n",
        "2) Spearman rank correlation: Measures monotonic relationships (not necessarily linear).\n",
        "\n",
        "3) Kendall Tau: Measures the strength of dependence between two variables.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*  A negative correlation means that the two variables move in opposite directions. In other words, when the value of one variable increases, the value of the other tends to decrease. This is represented by a correlation coefficient less than 0 and greater than –1.\n",
        "\n",
        "The closer the coefficient is to –1, the stronger the negative correlation. A value of –1 indicates a perfect inverse relationship.\n",
        "\n",
        "Negative correlation is especially important when analyzing features in a dataset. For example, in health-related datasets, you might observe that as physical activity increases, body fat percentage decreases, showing a negative correlation.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8iZUgrkdYmdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Define Machine Learning. What are the main components in Machine Learning?\n",
        " *  Machine Learning (ML) is a subset of artificial intelligence (AI) that enables systems to learn patterns from data and make decisions or predictions without being explicitly programmed for every task. It focuses on developing algorithms that improve automatically through experience.\n",
        "\n",
        "Rather than hardcoding logic for every possible scenario, machine learning systems identify complex patterns in data and adapt their behavior over time.\n",
        "\n",
        "At its core, ML is about building mathematical models based on data to perform specific tasks like classification, regression, clustering, and more.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Main Components in Machine Learning:\n",
        "\n",
        "* Dataset: The collection of data used for training and testing. It includes both input features (independent variables) and outputs or labels (dependent variables in supervised learning).\n",
        "\n",
        "* Features (Input Variables): These are the measurable properties or characteristics of the data used to make predictions. Feature engineering often enhances the quality of these inputs.\n",
        "\n",
        "* Model (Algorithm): The mathematical structure or function used to map inputs to outputs. Examples include Decision Trees, Linear Regression, Neural Networks, etc.\n",
        "\n",
        "* Loss Function: A function that measures how far the model’s prediction is from the actual target value. It quantifies error, guiding the model during training.\n",
        "\n",
        "* Optimizer: An algorithm that updates the model’s parameters to minimize the loss function. Common optimizers include Gradient Descent and its variants.\n",
        "\n",
        "* Training Process: The stage where the model learns patterns from the input data by adjusting its internal parameters to reduce error.\n",
        "\n",
        "* Evaluation Metrics: Metrics used to assess the performance of the trained model, such as accuracy, precision, recall, mean squared error, etc.\n",
        "\n",
        "* Testing and Validation: The phase where the trained model is applied to unseen data to check how well it generalizes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eWqtzSW5a1SW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  How does loss value help in determining whether the model is good or not?\n",
        " *   The loss value in machine learning is a numerical representation of how far off the model’s predictions are from the actual target values. It is calculated using a loss function, which measures the difference between predicted outputs and the actual outputs for a given set of inputs.\n",
        "\n",
        "A lower loss value means the model is making predictions that are closer to the actual values, while a higher loss value indicates poor predictions. Therefore, the loss value is a direct indicator of model performance during training and sometimes during testing too."
      ],
      "metadata": {
        "id": "glya1h2ocQ1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are Continuous and Categorical Variables?\n",
        "\n",
        " *  In machine learning and statistics, variables are the attributes or characteristics of data. These variables can be classified into two main types based on the nature of their values:\n",
        "\n",
        "* Continuous Variables: These are numerical variables that can take an infinite number of values within a given range. They are measurable quantities and can have decimal values. Examples include height, temperature, weight, age, and price. They support mathematical operations like addition, subtraction, averaging, etc.\n",
        "\n",
        "* Categorical Variables: These are variables that represent categories or groups. They contain a finite number of distinct values and are often non-numerical. Categorical data can be:\n",
        "\n",
        "1)  Nominal (no inherent order, e.g., gender, color, city)\n",
        "\n",
        "2) Ordinal (ordered categories, e.g., low, medium, high)\n",
        "\n"
      ],
      "metadata": {
        "id": "63p4SaIQdV8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        " *  Handling categorical variables in machine learning refers to the process of converting non-numeric data (categories or labels) into a numerical format that machine learning algorithms can understand and use effectively.\n",
        "\n",
        "Since most ML models require numeric input, this transformation is essential for training and prediction. The chosen method depends on the type of categorical data—nominal (unordered) or ordinal (ordered).\n",
        "\n",
        "\n",
        "Common Techniques to Handle Categorical Variables:\n",
        "\n",
        "* Label Encoding: Converts each category into a unique integer (0, 1, 2…). Best for ordinal variables.\n",
        "\n",
        "* One-Hot Encoding: Creates binary (0/1) columns for each category. Best for nominal data.\n",
        "\n",
        "* Ordinal Encoding: Manually assigns numbers to categories based on order or hierarchy.\n",
        "\n",
        "* Binary Encoding / Hashing / Target Encoding (Advanced): Used for high-cardinality features (e.g., hundreds of categories). Balances dimensionality and information content.\n",
        "\n"
      ],
      "metadata": {
        "id": "2oFu0DNveVxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?\n",
        "\n",
        " *  A dataset is typically divided into two main parts: the training dataset and the testing dataset. This separation helps evaluate the model's ability to generalize to unseen data.\n",
        "\n",
        "* Training Dataset: This is the portion of the data used to train the machine learning model. The algorithm learns patterns, relationships, and features from this subset by adjusting internal parameters (like weights).\n",
        "\n",
        "* Testing Dataset: This is a separate portion of the data that is not seen by the model during training. It is used to evaluate the model’s performance and check how well it generalizes to new, unseen inputs.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LQAvqLlifJ3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  What is sklearn.preprocessing?\n",
        "\n",
        " *   The sklearn.preprocessing module is part of the popular machine learning library scikit-learn and provides essential tools for transforming and preparing data for use with machine learning models. The primary aim of preprocessing is to improve the performance and accuracy of machine learning algorithms by ensuring that the data is in an appropriate format. This module includes a range of functionalities such as scaling numerical features, encoding categorical variables, handling missing data, and creating polynomial features, all of which are critical for the effective application of machine learning models."
      ],
      "metadata": {
        "id": "_GeLvMQvhLvs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  What is a Test set?\n",
        " *   A test set is a subset of the dataset that is used to evaluate the performance of a trained machine learning model. The test set contains data that the model has never seen before during the training process. Its primary purpose is to assess how well the model generalizes to new, unseen data, providing a more accurate reflection of the model's ability to perform in real-world scenarios.\n",
        "\n",
        "The test set serves as a final checkpoint after training and validation. While the training set is used to train the model and the validation set is used to tune model parameters, the test set is strictly reserved for evaluating the model's final performance. This separation ensures that the model is not overfitting to the training data and can make predictions effectively on new, unseen data."
      ],
      "metadata": {
        "id": "oBH_8eD-hTO-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  How do we split data for model fitting (training and testing) in Python?\n",
        " *   splitting the data into training and testing sets is a fundamental step that ensures the model is evaluated on data it hasn't seen before, helping to assess its generalization ability. The process of splitting the data typically involves dividing the dataset into at least two subsets: the training set and the test set.\n",
        "\n",
        " * Training Set: This subset is used to train the machine learning model. The model learns the underlying patterns and relationships in the data from this set.\n",
        "\n",
        "* Test Set: This subset is used to evaluate the performance of the model after it has been trained. The goal is to ensure the model performs well on unseen data, which helps to understand how it will generalize to real-world data.\n",
        "\n",
        "\n",
        "Most common method for splitting data is using the train_test_split function from the sklearn.model_selection module. This function randomly splits the dataset into training and testing subsets."
      ],
      "metadata": {
        "id": "PHp7PGDBiGiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. How do you approach a Machine Learning Problem?\n",
        "\n",
        " *  Approaching a machine learning problem involves several critical steps that guide the process from problem understanding to model deployment. The steps can vary depending on the problem, but they typically follow a general workflow:\n",
        "\n",
        "* Problem Definition: Clearly define the problem you are trying to solve.\n",
        "\n",
        "* Data Collection and Understanding: Collect the data that will be used to train and evaluate the model.\n",
        "\n",
        "* Data Preprocessing: Clean and preprocess the data by handling missing values, encoding categorical variables, scaling or normalizing features, and addressing any data imbalances or outliers.\n",
        "\n",
        "* Data Splitting: Split the data into training, validation, and test sets.\n",
        "\n",
        "* Model Selection: Choose a suitable machine learning algorithm based on the problem type.\n",
        "\n",
        "* Model Training: Train the model on the training data.\n",
        "\n",
        "* Model Evaluation: Evaluate the model using the test set to check how well it generalizes to unseen data.\n",
        "\n",
        "* Hyperparameter Tuning: Adjust the hyperparameters of the model to improve its performance.\n",
        "\n",
        "* Model Interpretation and Deployment: Interpret the model’s results, understand its behavior, and check for overfitting or underfitting."
      ],
      "metadata": {
        "id": "zVPPRoq-jJmi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Why do we have to perform EDA before fitting a model to the data?\n",
        " *  Exploratory Data Analysis (EDA) is the process of thoroughly examining, visualizing, and summarizing a dataset before applying any machine learning model. The main objective of EDA is to understand the structure, patterns, and quality of the data in order to make informed decisions about data preprocessing, feature engineering, and model selection.\n",
        "\n",
        "EDA acts as a bridge between raw data and model building. It helps identify underlying trends, anomalies, missing values, outliers, and relationships between variables. Without performing EDA, we risk feeding incorrect or misleading information into the model, which can lead to inaccurate predictions and poor generalization on unseen data."
      ],
      "metadata": {
        "id": "H356itTskLIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.  What is correlation?\n",
        "*  Correlation is a statistical measure that expresses the extent to which two variables are linearly related. It indicates whether an increase or decrease in one variable would lead to an increase or decrease in another. The value of correlation lies between -1 and +1, where:\n",
        "\n",
        " *  +1 indicates a perfect positive correlation\n",
        "\n",
        " *  -1 indicates a perfect negative correlation\n",
        "\n",
        " *  0 indicates no linear correlation\n",
        "\n",
        "\n",
        "There are several types of correlation:\n",
        "\n",
        "1) Pearson correlation: Measures linear correlation.\n",
        "\n",
        "2) Spearman rank correlation: Measures monotonic relationships (not necessarily linear).\n",
        "\n",
        "3) Kendall Tau: Measures the strength of dependence between two variables."
      ],
      "metadata": {
        "id": "uVuNw2BKkqYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.  What does negative correlation mean?\n",
        "  *  A negative correlation means that the two variables move in opposite directions. In other words, when the value of one variable increases, the value of the other tends to decrease. This is represented by a correlation coefficient less than 0 and greater than –1.\n",
        "\n",
        "The closer the coefficient is to –1, the stronger the negative correlation. A value of –1 indicates a perfect inverse relationship.\n",
        "\n",
        "Negative correlation is especially important when analyzing features in a dataset. For example, in health-related datasets, you might observe that as physical activity increases, body fat percentage decreases, showing a negative correlation.\n"
      ],
      "metadata": {
        "id": "Hg9HPYwIk8uO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How can you find correlation between variables in Python?\n",
        "\n",
        " *  Methods to Find Correlation in Python between variables:\n",
        "\n",
        "* Using pandas.corr() Method: This function computes the Pearson correlation coefficient by default. It can also compute Kendall and Spearman correlations by specifying the method parameter.\n",
        "\n",
        "* Visualizing Correlation with a Heatmap: A heatmap helps visually identify strong or weak correlations between variables using color intensities.\n",
        "\n",
        "* Choosing the Correlation Method:\n",
        "\n",
        "method='pearson': Measures linear correlation (most common)\n",
        "\n",
        "method='spearman': Based on rank, useful for non-linear but monotonic relationships\n",
        "\n",
        "method='kendall': Also based on rank, better for small datasets or ordinal data"
      ],
      "metadata": {
        "id": "Ssy6F6wwlLtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  What is causation? Explain difference between correlation and causation with an example.\n",
        " *  Causation refers to a direct relationship between two variables, where one variable is responsible for causing a change in the other. In other words, when a change in variable A directly results in a change in variable B, we say there is a cause-and-effect relationship between them.\n",
        "\n",
        "\n",
        "Difference between correlation and casuation:\n",
        "\n",
        "* Direction of Relationship: Correlation means that two variables move together in some way, either positively or negatively, whereas causation means that one variable directly causes the change in another.\n",
        "\n",
        "* Dependency: In correlation, variables are statistically related but may not depend on each other; in causation, one variable is dependent on the other due to a direct influence.\n",
        "\n",
        "* Evidence: Correlation can be measured using statistical formulas like Pearson’s coefficient, but causation requires scientific proof, experiments, or theoretical reasoning to confirm a cause-effect link.\n",
        "\n",
        "* Possibility of Misinterpretation: Correlation can occur due to coincidence or a third variable (confounder), while causation must be established by ruling out external influences.\n",
        "\n"
      ],
      "metadata": {
        "id": "uqcKmtPSmJw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.  What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        " * An optimizer is a crucial component in machine learning and deep learning algorithms, especially during the training of models. It is a mathematical function or algorithm that adjusts the model’s internal parameters (weights and biases) to minimize the loss function, which measures how far the model’s predictions are from the actual values.\n",
        "\n",
        "In other words, the optimizer's job is to improve the model's performance by iteratively updating the weights in such a way that the loss (error) is reduced as much as possible. This process is typically performed using techniques based on gradient descent or its variations.\n",
        "\n",
        "There are several types of optimizers used in machine learning which are:\n",
        "\n",
        "* Gradient Descent: This is the most basic form of optimization algorithm. It computes the gradient of the loss function using the entire training dataset and updates the model weights accordingly.\n",
        "\n",
        "Example: teacher evaluates the performance of an entire class before giving collective feedback. This approach gives an overall idea but is slow and resource-intensive, especially if the class is huge.\n",
        "\n",
        "* Stochastic Gradient Descent: SGD updates model parameters using only one data point at a time. While this introduces randomness and noise, it also makes the training process much faster.\n",
        "\n",
        "Ex: a coach who corrects an athlete after every single move instead of waiting for the whole session to end. Feedback is quick and frequent, but might sometimes be based on noisy or misleading observations.\n",
        "\n",
        "* Mini-Batch Gradient Descent: A middle-ground between batch and SGD, this method uses small chunks or batches of data (like 32 or 64 samples) to compute updates. It balances accuracy and speed.\n",
        "\n",
        "Ex: A teacher divides the class into small groups and gives feedback to each group separately. This makes learning efficient and avoids overwhelming either the teacher or students.\n",
        "\n",
        "*  Momentum Optimizer: Momentum builds on previous updates by adding a “memory” of past gradients. It helps the model move faster in the right direction and prevents it from getting stuck in small dips or oscillating.\n",
        "\n",
        "ex: Imagine rolling a ball downhill. Even if it hits a small bump, its momentum helps it roll past the obstacle instead of stopping or turning around.\n",
        "\n",
        "* AdaGrad: AdaGrad adjusts the learning rate for each parameter individually. It reduces the learning rate over time for frequently updated parameters and gives more focus to rarely updated ones.\n",
        "\n",
        "Ex: Suppose you're studying different subjects. If you're good at math but weak in history, you start spending less time on math and more on history to balance your overall performance.\n",
        "\n",
        "*  RMSProp: RMSProp improves AdaGrad by keeping a moving average of squared gradients, preventing the learning rate from becoming too small.\n",
        "\n",
        "ex: Think of a smart manager who doesn’t overreact to every single feedback but makes decisions based on a running average of employee performance, allowing for better long-term improvement.\n",
        "\n",
        "* Adam: Adam is one of the most widely used optimizers today. It combines the advantages of Momentum and RMSProp, making it efficient, stable, and adaptable to different problems.\n",
        "\n",
        "ex: a top-tier mentor who keeps track of both your progress (momentum) and your mistakes (gradient strength), and customizes advice based on both. It leads to balanced, fast, and stable improvements."
      ],
      "metadata": {
        "id": "bmD-pFX9nhMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.  What is sklearn.linear_model ?\n",
        "\n",
        " *  sklearn.linear_model is a module in the Scikit-learn (sklearn) library that provides a collection of linear models used for both regression and classification tasks. These models work on the principle that the output or target variable can be expressed as a linear combination of input features. The module contains classes that implement various types of linear algorithms, allowing developers and researchers to apply mathematical models that assume a linear relationship between input and output."
      ],
      "metadata": {
        "id": "HkErqvh9qtHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.fit() do? What arguments must be given?\n",
        " *  In machine learning using libraries like Scikit-learn, the model.fit() function is used to train a machine learning model on a given dataset. This is one of the most important steps in the model-building process.\n",
        "\n",
        "When you create a model (for example, using LinearRegression(), DecisionTreeClassifier(), or any other algorithm), the model is only a blank container—it doesn't know anything about your data yet. The fit() method fills that container by feeding in the data, allowing the model to learn the relationships or patterns between input features and target outputs.\n",
        "\n",
        "Necessary arguments are:\n",
        "\n",
        "*  X (Features/Input Data): A 2D array-like structure (such as list of lists, NumPy array, or pandas DataFrame) containing independent variables or features.\n",
        "Each row is one data point; each column is one feature.\n",
        "\n",
        "* y (Target/Labels): A 1D array-like structure (such as list, NumPy array, or pandas Series) containing the dependent variable or the value we want to predict.\n",
        "\n",
        "It should have the same number of entries as rows in X, where each entry corresponds to the correct output for the input in the same row."
      ],
      "metadata": {
        "id": "jl06kU6_vIXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.  What does model.predict() do? What arguments must be given?\n",
        " *  After a machine learning model has been trained using the model.fit() method, the model.predict() function is used to make predictions on new, unseen data. This method allows the model to apply the patterns it has learned during training to estimate or classify outcomes for new inputs.\n",
        "\n",
        " Necessary arguments are:\n",
        "\n",
        "*  X_new (Input Data for Prediction):\n",
        "\n",
        "This is the only required argument.\n",
        "\n",
        "It must be a 2D array-like structure (e.g., a list of lists, NumPy array, or pandas DataFrame), similar in structure to the training data used in model.fit().\n",
        "\n",
        "It should include the same number of features/columns as the training data, but can have any number of rows (i.e., data points).\n",
        "\n"
      ],
      "metadata": {
        "id": "uFcKz06zwbrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What are Continuous and Categorical Variables?\n",
        "\n",
        " *  In machine learning and statistics, variables are the attributes or characteristics of data. These variables can be classified into two main types based on the nature of their values:\n",
        "\n",
        "* Continuous Variables: These are numerical variables that can take an infinite number of values within a given range. They are measurable quantities and can have decimal values. Examples include height, temperature, weight, age, and price. They support mathematical operations like addition, subtraction, averaging, etc.\n",
        "\n",
        "* Categorical Variables: These are variables that represent categories or groups. They contain a finite number of distinct values and are often non-numerical. Categorical data can be:\n",
        "\n",
        "1)  Nominal (no inherent order, e.g., gender, color, city)\n",
        "\n",
        "2) Ordinal (ordered categories, e.g., low, medium, high)\n",
        "\n"
      ],
      "metadata": {
        "id": "L_UZJDLpw8LD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.  What is feature scaling? How does it help in Machine Learning?\n",
        " *  Feature scaling is a technique in machine learning used to standardize or normalize the range of independent variables (features) in a dataset. In many real-world datasets, features often have different units, magnitudes, or ranges. For example, age may range from 0 to 100, while income may range from thousands to millions. This difference in scale can create imbalance and lead to biased results during model training.\n",
        "\n",
        "Feature scaling transforms features to a common scale without distorting their underlying relationships. The goal is to ensure that all features contribute equally to the learning process, especially in algorithms that are sensitive to the magnitude of values.\n",
        "\n",
        "Its importance in machine learning:\n",
        "\n",
        "* Improves Model Accuracy: In algorithms like logistic regression, support vector machines, and neural networks, large differences in feature magnitudes can slow down or mislead the optimization process. Feature scaling helps the model converge faster and learn more effectively.\n",
        "\n",
        "* Removes Bias Toward Higher-Scale Features: Algorithms such as K-Nearest Neighbors (KNN) or K-Means clustering use distance metrics like Euclidean distance. If features are not scaled, the algorithm may be biased toward features with larger numerical ranges, even if those features are less important.\n",
        "\n",
        "* Enhances Interpretability of Coefficients:\n",
        "In linear models, scaled features allow for better interpretation and comparison of feature importance.\n",
        "\n",
        "* Prevents Numerical Instability:\n",
        "Unscaled data can result in unstable gradients or matrix operations (like covariance matrices in PCA), leading to poor training results or even runtime errors."
      ],
      "metadata": {
        "id": "zAENG728xIeZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.  How do we perform scaling in Python?\n",
        " *  Feature scaling is commonly performed using the sklearn.preprocessing module from the Scikit-learn library. This module provides various tools to scale or normalize numerical data so that features contribute equally to a machine learning model.\n",
        "\n",
        "Scaling is generally performed after splitting the data into training and testing sets and before feeding the data into the model. The purpose is to bring all features to a comparable scale, ensuring better performance for algorithms that are sensitive to feature magnitudes.\n",
        "\n",
        "Some methods in python to perform feature scaling:\n",
        "\n",
        "* Min-Max Scaling (Normalization)\n",
        "This method scales features to a fixed range, usually [0, 1].\n",
        "\n",
        "* Standardization (Z-score Normalization)\n",
        "This method transforms features to have a mean of 0 and standard deviation of 1.\n",
        "\n",
        "* MaxAbs Scaling\n",
        "This scales each feature by its maximum absolute value, useful for sparse data.\n",
        "\n",
        "* Robust Scaling\n",
        "This method uses the median and interquartile range, making it robust to outliers."
      ],
      "metadata": {
        "id": "azyRAYM-yGcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.  What is sklearn.preprocessing?\n",
        "\n",
        " *   The sklearn.preprocessing module is part of the popular machine learning library scikit-learn and provides essential tools for transforming and preparing data for use with machine learning models. The primary aim of preprocessing is to improve the performance and accuracy of machine learning algorithms by ensuring that the data is in an appropriate format. This module includes a range of functionalities such as scaling numerical features, encoding categorical variables, handling missing data, and creating polynomial features, all of which are critical for the effective application of machine learning models."
      ],
      "metadata": {
        "id": "yYHzitvlys-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.  How do we split data for model fitting (training and testing) in Python?\n",
        " *   splitting the data into training and testing sets is a fundamental step that ensures the model is evaluated on data it hasn't seen before, helping to assess its generalization ability. The process of splitting the data typically involves dividing the dataset into at least two subsets: the training set and the test set.\n",
        "\n",
        " * Training Set: This subset is used to train the machine learning model. The model learns the underlying patterns and relationships in the data from this set.\n",
        "\n",
        "* Test Set: This subset is used to evaluate the performance of the model after it has been trained. The goal is to ensure the model performs well on unseen data, which helps to understand how it will generalize to real-world data.\n",
        "\n",
        "\n",
        "Most common method for splitting data is using the train_test_split function from the sklearn.model_selection module. This function randomly splits the dataset into training and testing subsets."
      ],
      "metadata": {
        "id": "xCM7N1MUy3Gt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26.  Explain data encoding?\n",
        " *  Data encoding is the process of converting categorical data into numerical values so that it can be used effectively by machine learning algorithms. Since most ML models work with numerical data, categorical variables—such as names, labels, or categories—must be encoded into a numerical format before model training.\n",
        "\n",
        "Encoding ensures that the algorithm understands and processes categorical inputs correctly without misinterpreting them as ordinal or mathematically related unless they truly are. It plays a crucial role in feature engineering, especially when handling textual or label-based data columns."
      ],
      "metadata": {
        "id": "t-sA6Gu_zEC2"
      }
    }
  ]
}